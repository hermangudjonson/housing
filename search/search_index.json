{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Housing Project","text":"<p>housing routines and models</p>"},{"location":"API/load_prep/","title":"Load Prep Module","text":"<p>Routines for loading and preprocessing housing data</p>"},{"location":"API/load_prep/#housing.load_prep.NearZeroVarSelector","title":"NearZeroVarSelector","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>housing/load_prep.py</code> <pre><code>class NearZeroVarSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, freq_ratio=20, unique_pct=0.1):\n        self.freq_ratio = freq_ratio\n        self.unique_pct = unique_pct\n\n    def fit(self, X, y=None):\n        self.nzvar_ = near_zero_var(\n            X, freq_ratio=self.freq_ratio, unique_pct=self.unique_pct\n        )\n        return self\n\n    def transform(self, X):\n        return X.loc[:, ~self.nzvar_]\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.NearZeroVarSelector.freq_ratio","title":"freq_ratio  <code>instance-attribute</code>","text":"<pre><code>freq_ratio = freq_ratio\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.NearZeroVarSelector.unique_pct","title":"unique_pct  <code>instance-attribute</code>","text":"<pre><code>unique_pct = unique_pct\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.NearZeroVarSelector.__init__","title":"__init__","text":"<pre><code>__init__(freq_ratio=20, unique_pct=0.1)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def __init__(self, freq_ratio=20, unique_pct=0.1):\n    self.freq_ratio = freq_ratio\n    self.unique_pct = unique_pct\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.NearZeroVarSelector.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def fit(self, X, y=None):\n    self.nzvar_ = near_zero_var(\n        X, freq_ratio=self.freq_ratio, unique_pct=self.unique_pct\n    )\n    return self\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.NearZeroVarSelector.transform","title":"transform","text":"<pre><code>transform(X)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def transform(self, X):\n    return X.loc[:, ~self.nzvar_]\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.raw_train","title":"raw_train","text":"<pre><code>raw_train(input_dir=INPUT_DIR)\n</code></pre> <p>Load housing raw training data</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>             (<code>Path | str</code>, default:                 <code>INPUT_DIR</code> )         \u2013          <p>directory containing csv data, by default INPUT_DIR</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[DataFrame, Series]</code>         \u2013          <p>tuple containing:  - 1460 x 79 training dataframe with index Id  - 1460 target data series with index Id and name SalePrice</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def raw_train(input_dir: Path | str = INPUT_DIR):\n    \"\"\"Load housing raw training data\n\n    Parameters\n    ----------\n    input_dir : Path | str, optional\n        directory containing csv data, by default INPUT_DIR\n\n    Returns\n    -------\n    tuple[pd.DataFrame, pd.Series]\n        tuple containing:\n         - 1460 x 79 training dataframe with index Id\n         - 1460 target data series with index Id and name SalePrice\n    \"\"\"\n    input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir\n\n    raw_train_df = pd.read_csv(input_dir / \"train.csv\", index_col=\"Id\")\n    train_df = raw_train_df.drop(columns=\"SalePrice\")\n    target_ds = raw_train_df[\"SalePrice\"]\n\n    return train_df, target_ds\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.raw_test","title":"raw_test","text":"<pre><code>raw_test(input_dir=INPUT_DIR)\n</code></pre> <p>Load housing raw test data</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>             (<code>Path | str</code>, default:                 <code>INPUT_DIR</code> )         \u2013          <p>directory containing csv data, by default INPUT_DIR</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>1459 x 79 test dataframe with index Id</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def raw_test(input_dir: Path | str = INPUT_DIR):\n    \"\"\"Load housing raw test data\n\n    Parameters\n    ----------\n    input_dir : Path | str, optional\n        directory containing csv data, by default INPUT_DIR\n\n    Returns\n    -------\n    pd.DataFrame\n        1459 x 79 test dataframe with index Id\n    \"\"\"\n    input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir\n\n    test_df = pd.read_csv(input_dir / \"test.csv\", index_col=\"Id\")\n    return test_df\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.example_submission","title":"example_submission","text":"<pre><code>example_submission(input_dir=INPUT_DIR)\n</code></pre> <p>Load housing example submission</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>             (<code>Path | str</code>, default:                 <code>INPUT_DIR</code> )         \u2013          <p>directory containing csv data, by default INPUT_DIR</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Series</code>         \u2013          <p>1459 series containing example predictions on test data</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def example_submission(input_dir: Path | str = INPUT_DIR):\n    \"\"\"Load housing example submission\n\n    Parameters\n    ----------\n    input_dir : Path | str, optional\n        directory containing csv data, by default INPUT_DIR\n\n    Returns\n    -------\n    pd.Series\n        1459 series containing example predictions on test data\n    \"\"\"\n    input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir\n\n    submission_ds = pd.read_csv(\n        input_dir / \"sample_submission.csv\", index_col=\"Id\"\n    ).squeeze()\n    return submission_ds\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.transform_target","title":"transform_target","text":"<pre><code>transform_target(y)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def transform_target(y):\n    return np.log(y)\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.inv_transform_target","title":"inv_transform_target","text":"<pre><code>inv_transform_target(y)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def inv_transform_target(y):\n    return np.exp(y)\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.near_zero_var","title":"near_zero_var","text":"<pre><code>near_zero_var(X, *, freq_ratio=20, unique_pct=0.1, plot=False, return_stats=False)\n</code></pre> <p>Evaluates each feature for near zero variance.</p> <p>Feature is near zero variance if ratio of largest freq value to 2nd largest freq value is high and percentage of unique values is low</p> <p>Parameters:</p> <ul> <li> <code>X</code>             (<code>DataFrame</code>)         \u2013          </li> <li> <code>freq_ratio</code>             (<code>int</code>, default:                 <code>20</code> )         \u2013          <p>threshold for ratio of largest freq value to 2nd largest freq value, by default 20</p> </li> <li> <code>unique_pct</code>             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>threshold for percentage of unique values to total samples, by default 0.1</p> </li> <li> <code>plot</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>plot feature stats, by default False</p> </li> <li> <code>return_stats</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>return dataframe with stats if True, otherwise just return boolean series, by default False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Series | DataFrame</code>         \u2013          <p>boolean series indicating near zero var, otherwise dataframe with stats</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def near_zero_var(\n    X: pd.DataFrame, *, freq_ratio=20, unique_pct=0.1, plot=False, return_stats=False\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Evaluates each feature for near zero variance.\n\n    Feature is near zero variance if ratio of largest freq value\n    to 2nd largest freq value is high and percentage of unique values is low\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n    freq_ratio : int, optional\n        threshold for ratio of largest freq value to 2nd largest freq value, by default 20\n    unique_pct : float, optional\n        threshold for percentage of unique values to total samples, by default 0.1\n    plot : bool, optional\n        plot feature stats, by default False\n    return_stats : bool, optional\n        return dataframe with stats if True, otherwise just return boolean series, by default False\n\n    Returns\n    -------\n    pd.Series | pd.DataFrame\n        boolean series indicating near zero var, otherwise dataframe with stats\n    \"\"\"\n\n    def _freq_ratio(col):\n        col_counts = col.value_counts(dropna=False)\n        return (\n            col_counts.iloc[0] / col_counts.iloc[1] if len(col_counts) &gt; 1 else math.inf\n        )\n\n    def _unique_pct(col):\n        return col.nunique() / len(col)\n\n    stat_agg = X.agg([_freq_ratio, _unique_pct], axis=0).T\n    nzvar = (stat_agg._freq_ratio &gt; freq_ratio) &amp; (stat_agg._unique_pct &lt; unique_pct)\n    stat_agg[\"near_zero_var\"] = nzvar\n    logger.info(\"{total} columns with near zero var\", total=sum(nzvar))\n    if return_stats:\n        nzvar = stat_agg\n\n    if plot:\n        sns.scatterplot(stat_agg, x=\"_unique_pct\", y=\"_freq_ratio\", hue=\"near_zero_var\")\n        plt.xscale(\"log\")\n        plt.yscale(\"log\")\n        plt.show()\n\n    return nzvar\n</code></pre>"},{"location":"API/load_prep/#housing.load_prep.preprocess_pipe","title":"preprocess_pipe","text":"<pre><code>preprocess_pipe()\n</code></pre> <p>Pipeline that filters near-zero variance columns and scales numeric data</p> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def preprocess_pipe():\n    \"\"\"Pipeline that filters near-zero variance columns and scales numeric data\n\n    Returns\n    -------\n    Pipeline\n    \"\"\"\n    cat_scale = make_column_transformer(\n        (\n            # convert any object columns to categoricals\n            FunctionTransformer(\n                lambda X: X.astype(\"category\"), feature_names_out=\"one-to-one\"\n            ),\n            make_column_selector(dtype_include=\"object\"),\n        ),\n        # power transform all numeric columns\n        remainder=make_pipeline(StandardScaler(), PowerTransformer()).set_output(\n            transform=\"pandas\"\n        ),\n        verbose_feature_names_out=False\n    ).set_output(transform='pandas')\n\n    return make_pipeline(NearZeroVarSelector(), cat_scale)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>housing<ul> <li>automl</li> <li>gpu_check</li> <li>lightgbm_tune</li> <li>load_prep</li> <li>model</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/housing/","title":"housing","text":"<p>housing routines and models</p>"},{"location":"reference/housing/automl/","title":"automl","text":"<p>Modelling with Autogluon and FlaML</p> <p>Use budget-based model search on each platform. Compare raw training data vs minor pre-processing.</p>"},{"location":"reference/housing/automl/#housing.automl.AGProxy","title":"AGProxy","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> Source code in <code>housing/automl.py</code> <pre><code>class AGProxy(BaseEstimator, RegressorMixin):\n    def __init__(self, **params):\n        self.params = params\n\n    def get_params(self, deep=True):\n        return self.params\n\n    def set_params(self, **params):\n        self.params.update(params)\n\n    def fit(self, X, y):\n        tab_data = TabularDataset(pd.concat([X, pd.DataFrame(y)], axis=1))\n\n        self.params.update({\"label\": y.name})\n        # partition between model and fit params\n        model_params = self.params.copy()\n        fit_keys = {\"time_limit\", \"presets\"}\n        fit_params = {\n            k: model_params.pop(k) for k in self.params.keys() if k in fit_keys\n        }\n        self.estimator_ = TabularPredictor(**model_params)\n\n        self.estimator_.fit(tab_data, **fit_params)\n        return self\n\n    def __getattr__(self, name):\n        \"\"\"dispatch other methods to estimator\"\"\"\n        return getattr(self.estimator_, name)\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.AGProxy.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params = params\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.AGProxy.__init__","title":"__init__","text":"<pre><code>__init__(**params)\n</code></pre> Source code in <code>housing/automl.py</code> <pre><code>def __init__(self, **params):\n    self.params = params\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.AGProxy.get_params","title":"get_params","text":"<pre><code>get_params(deep=True)\n</code></pre> Source code in <code>housing/automl.py</code> <pre><code>def get_params(self, deep=True):\n    return self.params\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.AGProxy.set_params","title":"set_params","text":"<pre><code>set_params(**params)\n</code></pre> Source code in <code>housing/automl.py</code> <pre><code>def set_params(self, **params):\n    self.params.update(params)\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.AGProxy.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> Source code in <code>housing/automl.py</code> <pre><code>def fit(self, X, y):\n    tab_data = TabularDataset(pd.concat([X, pd.DataFrame(y)], axis=1))\n\n    self.params.update({\"label\": y.name})\n    # partition between model and fit params\n    model_params = self.params.copy()\n    fit_keys = {\"time_limit\", \"presets\"}\n    fit_params = {\n        k: model_params.pop(k) for k in self.params.keys() if k in fit_keys\n    }\n    self.estimator_ = TabularPredictor(**model_params)\n\n    self.estimator_.fit(tab_data, **fit_params)\n    return self\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.AGProxy.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>dispatch other methods to estimator</p> Source code in <code>housing/automl.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"dispatch other methods to estimator\"\"\"\n    return getattr(self.estimator_, name)\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.fit_flaml","title":"fit_flaml","text":"<pre><code>fit_flaml(preprocess=False, time_budget=1, refit_time_budget=1, outdir=None)\n</code></pre> Source code in <code>housing/automl.py</code> <pre><code>def fit_flaml(preprocess=False, time_budget=1, refit_time_budget=1, outdir=None):\n    automl_params = {\n        \"time_budget\": time_budget,  # in seconds\n        \"early_stop\": True,\n        \"metric\": \"mse\",\n        \"task\": \"regression\",\n        \"ensemble\": True,\n        \"log_file_name\": str(utils.WORKING_DIR / outdir / \"housing_flaml.log\")\n        if outdir\n        else \"\",\n    }\n\n    if preprocess:\n        flaml_reg = model.get_reg_pipeline(\n            reg_strategy=\"flaml\", reg_params=automl_params, as_category=True\n        )\n    else:\n        flaml_reg = flaml.AutoML(**automl_params)\n\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n\n    sfold = KFold(n_splits=5, shuffle=True, random_state=1234)\n\n    flaml_reg.fit(X, y)\n\n    fitted_params = automl_params | {\n        \"time_budget\": refit_time_budget,  # in seconds\n        \"log_file_name\": \"\",\n        \"starting_points\": flaml_reg[-1].best_config_per_estimator\n        if preprocess\n        else flaml_reg.best_config_per_estimator,\n    }\n\n    # cv results evaluation\n    if preprocess:\n        flaml_best_model = model.get_reg_pipeline(\n            reg_strategy=\"flaml\",\n            reg_params=fitted_params,\n            as_category=True,\n        )\n    else:\n        flaml_best_model = flaml.AutoML(**fitted_params)\n\n    cv_results = model.cv_with_validation(\n        flaml_best_model,\n        X,\n        y,\n        sfold,\n        callbacks=model.common_cv_callbacks(),\n    )\n    cv_results_df = _cv_results_df(cv_results)\n\n    # make submission predictions\n    X_test = load_prep.raw_test()\n    flaml_predict = pd.Series(\n        load_prep.inv_transform_target(flaml_reg.predict(X_test)),\n        name=\"SalePrice\",\n        index=X_test.index,\n    )\n\n    if outdir is not None:\n        # save flaml model\n        mlflow.sklearn.save_model(flaml_reg, utils.WORKING_DIR / outdir / \"flaml_model\")\n        cv_results_df.to_csv(utils.WORKING_DIR / outdir / \"flaml_best_eval_test.csv\")\n\n        # save submission predictions\n        flaml_predict.to_csv(\"flaml_predict.csv\")\n\n    return flaml_reg\n</code></pre>"},{"location":"reference/housing/automl/#housing.automl.fit_autogluon","title":"fit_autogluon","text":"<pre><code>fit_autogluon(preprocess=False, time_limit=1, presets='best_quality', outdir=None)\n</code></pre> Source code in <code>housing/automl.py</code> <pre><code>def fit_autogluon(preprocess=False, time_limit=1, presets=\"best_quality\", outdir=None):\n    ag_params = {\n        \"problem_type\": \"regression\",\n        \"eval_metric\": \"root_mean_squared_error\",\n        \"path\": str(utils.WORKING_DIR / outdir / \"ag_fit_output\")\n        if outdir is not None\n        else None,\n        \"time_limit\": time_limit,\n        \"presets\": presets,\n    }\n\n    if preprocess:\n        ag_reg = model.get_reg_pipeline(\n            reg_strategy=\"autogluon\", reg_params=ag_params, as_category=True\n        )\n    else:\n        ag_reg = AGProxy(**ag_params)\n\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n\n    ag_reg.fit(X, y)\n    ag_summary = (ag_reg[-1] if preprocess else ag_reg).fit_summary()\n\n    # make submission predictions\n    X_test = load_prep.raw_test()\n    ag_predict = pd.Series(\n        load_prep.inv_transform_target(ag_reg.predict(X_test)),\n        name=\"SalePrice\",\n        index=X_test.index,\n    )\n\n    if outdir is not None:\n        # save flaml model\n        mlflow.sklearn.save_model(ag_reg, utils.WORKING_DIR / outdir / \"ag_model\")\n        ag_summary[\"leaderboard\"].to_csv(\n            utils.WORKING_DIR / outdir / \"ag_leaderboard.csv\"\n        )\n\n        # save submission predictions\n        ag_predict.to_csv(\"autogluon_predict.csv\")\n\n    return ag_reg\n</code></pre>"},{"location":"reference/housing/gpu_check/","title":"gpu_check","text":"<p>Simple routine to test GPU functionality</p>"},{"location":"reference/housing/gpu_check/#housing.gpu_check.train_lightgbm","title":"train_lightgbm","text":"<pre><code>train_lightgbm(X, y, device='gpu')\n</code></pre> Source code in <code>housing/gpu_check.py</code> <pre><code>def train_lightgbm(X, y, device=\"gpu\"):\n    lgbm_params = {\n        \"objective\": \"regression\",\n        \"max_bin\": 63,\n        \"num_leaves\": 255,\n        \"n_estimators\": 50,\n        \"learning_rate\": 0.1,\n        \"device\": device,\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n        \"verbose\": 1,\n    }\n\n    lgbm_model = lgbm.LGBMRegressor(**lgbm_params)\n    lgbm_model.fit(X, y)\n    return lgbm_model\n</code></pre>"},{"location":"reference/housing/gpu_check/#housing.gpu_check.train_linear","title":"train_linear","text":"<pre><code>train_linear(X, y, use_cuml=True)\n</code></pre> Source code in <code>housing/gpu_check.py</code> <pre><code>def train_linear(X, y, use_cuml=True):\n    if use_cuml:\n        lin_model = cuml.linear_model.LinearRegression()\n    else:\n        lin_model = sklearn.linear_model.LinearRegression()\n    lin_model.fit(X, y)\n    return lin_model\n</code></pre>"},{"location":"reference/housing/gpu_check/#housing.gpu_check.compare_device","title":"compare_device","text":"<pre><code>compare_device(gpu_device='gpu')\n</code></pre> Source code in <code>housing/gpu_check.py</code> <pre><code>def compare_device(gpu_device=\"gpu\"):\n    X, y = make_regression(n_samples=1_000_000, n_features=100, n_informative=10)\n\n    logger.info(\"training linear cpu\")\n    print(timeit.repeat(lambda: train_linear(X, y, use_cuml=False), number=1))\n    logger.info(\"training linear gpu\")\n    print(timeit.repeat(lambda: train_linear(X, y, use_cuml=True), number=1))\n    logger.info(\"training lightgbm cpu\")\n    print(timeit.repeat(lambda: train_lightgbm(X, y, device=\"cpu\"), number=1))\n    logger.info(\"training lightgbm gpu\")\n    print(timeit.repeat(lambda: train_lightgbm(X, y, device=gpu_device), number=1))\n</code></pre>"},{"location":"reference/housing/gpu_check/#housing.gpu_check.train_tsne","title":"train_tsne","text":"<pre><code>train_tsne(X, use_cuml=True)\n</code></pre> Source code in <code>housing/gpu_check.py</code> <pre><code>def train_tsne(X, use_cuml=True):\n    # params should be valid for both sklearn and cuml\n    tsne_params = {\n        \"n_iter\": 10_000,\n        \"method\": \"fft\" if use_cuml else \"barnes_hut\",\n        \"init\": \"random\",\n        \"learning_rate\": 200,\n        \"verbose\": 6,\n    }\n    if use_cuml:\n        tsne_model = cuml.TSNE(**tsne_params)\n    else:\n        tsne_model = sklearn.manifold.TSNE(**tsne_params)\n\n    result = tsne_model.fit_transform(X)\n    logger.info((type(result), result.shape))\n    return pd.DataFrame(np.array(result), index=X.index, columns=[\"TSNE1\", \"TSNE2\"])\n</code></pre>"},{"location":"reference/housing/gpu_check/#housing.gpu_check.train_umap","title":"train_umap","text":"<pre><code>train_umap(X, use_cuml=True)\n</code></pre> Source code in <code>housing/gpu_check.py</code> <pre><code>def train_umap(X, use_cuml=True):\n    umap_params = {\"n_epochs\": 10_000, \"init\": \"random\", \"verbose\": True}\n    if use_cuml:\n        umap_model = cuml.UMAP(**umap_params)\n    else:\n        umap_model = umap.UMAP(**umap_params)\n\n    result = umap_model.fit_transform(X)\n    logger.info((type(result), result.shape))\n    return pd.DataFrame(np.array(result), index=X.index, columns=[\"UMAP1\", \"UMAP2\"])\n</code></pre>"},{"location":"reference/housing/gpu_check/#housing.gpu_check.compare_embedding","title":"compare_embedding","text":"<pre><code>compare_embedding(outdir=None)\n</code></pre> Source code in <code>housing/gpu_check.py</code> <pre><code>def compare_embedding(outdir=None):\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n    pca_pipe = model.get_pca_pipeline(cat_n_components=10, num_n_components=10)\n    pca_df = pca_pipe.fit_transform(X, y)\n\n    fit_time, embedding = {}, {}\n    logger.info(\"training tsne cpu\")\n    fit_time[\"tsne_cpu\"] = timeit.repeat(\n        lambda: train_tsne(pca_df, use_cuml=False), number=1\n    )\n    embedding[\"tsne_cpu\"] = train_tsne(pca_df, use_cuml=False)\n\n    logger.info(\"training tsne gpu\")\n    fit_time[\"tsne_gpu\"] = timeit.repeat(\n        lambda: train_tsne(pca_df, use_cuml=True), number=1\n    )\n    embedding[\"tsne_gpu\"] = train_tsne(pca_df, use_cuml=True)\n\n    logger.info(\"training umap cpu\")\n    fit_time[\"umap_cpu\"] = timeit.repeat(\n        lambda: train_umap(pca_df, use_cuml=False), number=1\n    )\n    embedding[\"umap_cpu\"] = train_umap(pca_df, use_cuml=False)\n\n    logger.info(\"training umap gpu\")\n    fit_time[\"umap_gpu\"] = timeit.repeat(\n        lambda: train_umap(pca_df, use_cuml=True), number=1\n    )\n    embedding[\"umap_gpu\"] = train_umap(pca_df, use_cuml=True)\n\n    print(fit_time)\n    if outdir is not None:\n        with open(utils.WORKING_DIR / outdir / \"embedding_fit_times.pkl\", \"wb\") as f:\n            cloudpickle.dump(fit_time, f)\n        with open(utils.WORKING_DIR / outdir / \"embedding.pkl\", \"wb\") as f:\n            cloudpickle.dump(embedding, f)\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/","title":"lightgbm_tune","text":"<p>LightGBM hyperparameter optimization with optuna</p> <p>Define optuna objective, study and runs. Additionally compare GPU fitting performance.</p> <p>tuning progression:</p> <ul> <li>n_estimators run time</li> <li>learning_rate sweep to evalute early stopping points</li> <li>broad parameter sweep</li> </ul>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.n_estimators_objective","title":"n_estimators_objective","text":"<pre><code>n_estimators_objective(trial, X, y, n_estimators=1000, device='cpu')\n</code></pre> <p>objective for n_estimators sample</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def n_estimators_objective(trial, X, y, n_estimators=1000, device=\"cpu\"):\n    \"\"\"objective for n_estimators sample\"\"\"\n    lgbm_params = {\n        \"objective\": \"regression\",\n        \"n_estimators\": n_estimators,  # time 1k trials\n        \"learning_rate\": 5e-3,\n        \"verbose\": -1,\n        # gpu settings\n        \"device\": device,\n        \"max_bin\": 63 if device == \"gpu\" else 255,\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n        # sampled params\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 7, 4095),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 100),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10, log=True),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n    }\n\n    sfold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1234)\n\n    reg_pipe = model.get_reg_pipeline(\n        reg_strategy=\"lightgbm\", reg_params=lgbm_params, as_category=True\n    )\n    cv_results = model.cv_with_validation(\n        reg_pipe,\n        X,\n        y,\n        sfold,\n        callbacks=model.common_cv_callbacks()\n        | {\"lgbm_metrics\": model.lgbm_fit_metrics},\n    )\n    cv_results_df = _cv_results_df(cv_results)\n\n    eval_test = cv_results_df.mean(numeric_only=True)\n    for k, v in eval_test.items():\n        trial.set_user_attr(k, v)\n    return eval_test[\"test_l2\"]\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.n_estimators_sample","title":"n_estimators_sample","text":"<pre><code>n_estimators_sample(n_trials=20, outdir='.', n_estimators=1000, device='cpu')\n</code></pre> <p>run optuna lightgbm n_estimators samples</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def n_estimators_sample(n_trials=20, outdir=\".\", n_estimators=1000, device=\"cpu\"):\n    \"\"\"run optuna lightgbm n_estimators samples\"\"\"\n    sql_file = f'sqlite:///{str(utils.WORKING_DIR / outdir / f\"lgbm_n_estimators_sample_{device}.db\")}'\n\n    study = optuna.create_study(\n        storage=sql_file,\n        load_if_exists=False,\n        study_name=\"lgbm_n_estimators\",\n        pruner=optuna.pruners.NopPruner(),\n        direction=\"minimize\",\n        sampler=optuna.samplers.RandomSampler(),\n    )\n\n    warnings.simplefilter(\"ignore\")  # to suppress multiple callback warning\n    # pre-load data for trials\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n    study.optimize(\n        functools.partial(\n            n_estimators_objective, X=X, y=y, n_estimators=n_estimators, device=device\n        ),\n        n_trials=n_trials,\n    )\n    warnings.resetwarnings()\n    return study\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.early_stopping_objective","title":"early_stopping_objective","text":"<pre><code>early_stopping_objective(trial, X, y, device='cpu')\n</code></pre> <p>objective for early stopping grid</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def early_stopping_objective(trial, X, y, device=\"cpu\"):\n    \"\"\"objective for early stopping grid\"\"\"\n    lgbm_params = {\n        \"objective\": \"regression\",\n        \"n_estimators\": 10_000,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1.0, log=True),\n        \"callbacks\": [lgbm.early_stopping(20, first_metric_only=True)],\n        \"verbose\": -1,\n        # gpu settings\n        \"device\": device,\n        \"max_bin\": 63 if device == \"gpu\" else 255,\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n        # sampled params\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 7, 4095),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 100),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10, log=True),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n    }\n\n    sfold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1234)\n\n    reg_pipe = model.get_reg_pipeline(\n        reg_strategy=\"lightgbm\", reg_params=lgbm_params, as_category=True\n    )\n    cv_results = model.cv_with_validation(\n        reg_pipe,\n        X,\n        y,\n        sfold,\n        callbacks=model.common_cv_callbacks()\n        | {\"lgbm_metrics\": model.lgbm_fit_metrics},\n    )\n    cv_results_df = _cv_results_df(cv_results)\n\n    eval_test = cv_results_df.mean(numeric_only=True)\n    for k, v in eval_test.items():\n        trial.set_user_attr(k, v)\n    return eval_test[\"test_l2\"]\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.early_stopping_sweep","title":"early_stopping_sweep","text":"<pre><code>early_stopping_sweep(n_trials=20, outdir='.', device='cpu')\n</code></pre> <p>run optuna lightgbm early stopping sweep</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def early_stopping_sweep(n_trials=20, outdir=\".\", device=\"cpu\"):\n    \"\"\"run optuna lightgbm early stopping sweep\"\"\"\n    sql_file = f'sqlite:///{str(utils.WORKING_DIR / outdir / f\"lgbm_early_stopping_sweep_{device}.db\")}'\n\n    study = optuna.create_study(\n        storage=sql_file,\n        load_if_exists=False,\n        study_name=\"lgbm_early_stopping\",\n        pruner=optuna.pruners.NopPruner(),\n        direction=\"minimize\",\n        sampler=optuna.samplers.RandomSampler(),\n    )\n\n    warnings.simplefilter(\"ignore\")  # to suppress multiple callback warning\n    # pre-load data for trials\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n    study.optimize(\n        functools.partial(early_stopping_objective, X=X, y=y, device=device),\n        n_trials=n_trials,\n    )\n    warnings.resetwarnings()\n    return study\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.broad_objective","title":"broad_objective","text":"<pre><code>broad_objective(trial, X, y, device='cpu')\n</code></pre> <p>objective for broad hpo</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def broad_objective(trial, X, y, device=\"cpu\"):\n    \"\"\"objective for broad hpo\"\"\"\n    lgbm_params = {\n        \"objective\": \"regression\",\n        \"n_estimators\": 2_000,\n        \"learning_rate\": 5e-2,\n        \"callbacks\": [\n            optuna.integration.LightGBMPruningCallback(\n                trial, \"l2\", valid_name=\"validation\"\n            ),\n            lgbm.early_stopping(20, first_metric_only=True),\n        ],\n        \"verbose\": -1,\n        # gpu settings\n        \"device\": device,\n        \"max_bin\": 63 if device == \"gpu\" else 255,\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n        # sampled params\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 7, 4095),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 100),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10, log=True),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n    }\n\n    sfold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1234)\n\n    reg_pipe = model.get_reg_pipeline(\n        reg_strategy=\"lightgbm\", reg_params=lgbm_params, as_category=True\n    )\n    cv_results = model.cv_with_validation(\n        reg_pipe,\n        X,\n        y,\n        sfold,\n        callbacks=model.common_cv_callbacks()\n        | {\"lgbm_metrics\": model.lgbm_fit_metrics},\n    )\n    cv_results_df = _cv_results_df(cv_results)\n\n    eval_test = cv_results_df.mean(numeric_only=True)\n    for k, v in eval_test.items():\n        trial.set_user_attr(k, v)\n    return eval_test[\"test_l2\"]\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.broad_hpo","title":"broad_hpo","text":"<pre><code>broad_hpo(n_trials=100, timeout=3600, outdir='.', device='cpu', prune=True)\n</code></pre> <p>run optuna lightgbm broad hpo</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def broad_hpo(n_trials=100, timeout=3600, outdir=\".\", device=\"cpu\", prune=True):\n    \"\"\"run optuna lightgbm broad hpo\"\"\"\n    if prune:\n        pruner = optuna.pruners.MedianPruner(\n            n_startup_trials=5, n_warmup_steps=200, interval_steps=50, n_min_trials=5\n        )\n    else:\n        pruner = optuna.pruners.NopPruner()\n\n    sql_file = (\n        f'sqlite:///{str(utils.WORKING_DIR / outdir / f\"lgbm_broad_hpo_{device}.db\")}'\n    )\n\n    study = optuna.create_study(\n        storage=sql_file,\n        load_if_exists=False,\n        study_name=\"lgbm_broad_hpo\",\n        pruner=pruner,\n        direction=\"minimize\",\n        sampler=optuna.samplers.TPESampler(),\n    )\n\n    warnings.simplefilter(\"ignore\")  # to suppress multiple callback warning\n    # pre-load data for trials\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n    study.optimize(\n        functools.partial(broad_objective, X=X, y=y, device=device),\n        n_trials=n_trials,\n        timeout=timeout,\n    )\n    warnings.resetwarnings()\n    return study\n</code></pre>"},{"location":"reference/housing/lightgbm_tune/#housing.lightgbm_tune.cv_best_trial","title":"cv_best_trial","text":"<pre><code>cv_best_trial(learning_rate=0.5, device='cpu', outdir=None)\n</code></pre> <p>Fit across CV folds with best LightGBM hyperparameters.</p> Source code in <code>housing/lightgbm_tune.py</code> <pre><code>def cv_best_trial(learning_rate=5e-1, device=\"cpu\", outdir=None):\n    \"\"\"Fit across CV folds with best LightGBM hyperparameters.\"\"\"\n    lgbm_params = {\n        \"objective\": \"regression\",\n        \"n_estimators\": 10_000,\n        \"learning_rate\": learning_rate,  # 5e-3 for final train\n        \"callbacks\": [lgbm.early_stopping(20, first_metric_only=True)],\n        \"verbose\": -1,\n        # gpu settings\n        \"device\": device,\n        \"max_bin\": 63 if device == \"gpu\" else 255,\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n    }\n    # best trial params\n    trial_params = {\n        \"num_leaves\": 11,\n        \"min_data_in_leaf\": 23,\n        \"lambda_l1\": 0.0009622039584818298,\n        \"lambda_l2\": 0.007667542167871574,\n        \"feature_fraction\": 0.5236846664479496,\n        \"bagging_fraction\": 0.9658064244332824,\n        \"bagging_freq\": 6,\n    }\n    lgbm_params = lgbm_params | trial_params\n\n    raw_train_df, target_ds = load_prep.raw_train()\n    X, y = raw_train_df, load_prep.transform_target(target_ds)\n\n    sfold = KFold(n_splits=5, shuffle=True, random_state=1234)\n\n    reg_pipe = model.get_reg_pipeline(\n        reg_strategy=\"lightgbm\", reg_params=lgbm_params, as_category=True\n    )\n    cv_results = model.cv_with_validation(\n        reg_pipe,\n        X,\n        y,\n        sfold,\n        callbacks=model.common_cv_callbacks()\n        | {\"lgbm_metrics\": model.lgbm_fit_metrics},\n    )\n    cv_results_df = _cv_results_df(cv_results)\n\n    # make submission predictions\n    X_test = load_prep.raw_test()\n    lgbm_predict = pd.Series(\n        _cvr_predict(cv_results, X_test), name=\"SalePrice\", index=X_test.index\n    )\n\n    if outdir is not None:\n        # pickle cv results\n        with open(utils.WORKING_DIR / outdir / \"lgbm_best_cv.pkl\", \"wb\") as f:\n            cloudpickle.dump(cv_results, f)\n        cv_results_df.to_csv(utils.WORKING_DIR / outdir / \"lgbm_best_eval_test.csv\")\n\n        # save submission predictions\n        lgbm_predict.to_csv(\"lgbm_cv_predict.csv\")\n\n    return cv_results\n</code></pre>"},{"location":"reference/housing/load_prep/","title":"load_prep","text":"<p>Routines for loading and preprocessing housing data</p>"},{"location":"reference/housing/load_prep/#housing.load_prep.NearZeroVarSelector","title":"NearZeroVarSelector","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>housing/load_prep.py</code> <pre><code>class NearZeroVarSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, freq_ratio=20, unique_pct=0.1):\n        self.freq_ratio = freq_ratio\n        self.unique_pct = unique_pct\n\n    def fit(self, X, y=None):\n        self.nzvar_ = near_zero_var(\n            X, freq_ratio=self.freq_ratio, unique_pct=self.unique_pct\n        )\n        return self\n\n    def transform(self, X):\n        return X.loc[:, ~self.nzvar_]\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.NearZeroVarSelector.freq_ratio","title":"freq_ratio  <code>instance-attribute</code>","text":"<pre><code>freq_ratio = freq_ratio\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.NearZeroVarSelector.unique_pct","title":"unique_pct  <code>instance-attribute</code>","text":"<pre><code>unique_pct = unique_pct\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.NearZeroVarSelector.__init__","title":"__init__","text":"<pre><code>__init__(freq_ratio=20, unique_pct=0.1)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def __init__(self, freq_ratio=20, unique_pct=0.1):\n    self.freq_ratio = freq_ratio\n    self.unique_pct = unique_pct\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.NearZeroVarSelector.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def fit(self, X, y=None):\n    self.nzvar_ = near_zero_var(\n        X, freq_ratio=self.freq_ratio, unique_pct=self.unique_pct\n    )\n    return self\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.NearZeroVarSelector.transform","title":"transform","text":"<pre><code>transform(X)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def transform(self, X):\n    return X.loc[:, ~self.nzvar_]\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.raw_train","title":"raw_train","text":"<pre><code>raw_train(input_dir=INPUT_DIR)\n</code></pre> <p>Load housing raw training data</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>             (<code>Path | str</code>, default:                 <code>INPUT_DIR</code> )         \u2013          <p>directory containing csv data, by default INPUT_DIR</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[DataFrame, Series]</code>         \u2013          <p>tuple containing:  - 1460 x 79 training dataframe with index Id  - 1460 target data series with index Id and name SalePrice</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def raw_train(input_dir: Path | str = INPUT_DIR):\n    \"\"\"Load housing raw training data\n\n    Parameters\n    ----------\n    input_dir : Path | str, optional\n        directory containing csv data, by default INPUT_DIR\n\n    Returns\n    -------\n    tuple[pd.DataFrame, pd.Series]\n        tuple containing:\n         - 1460 x 79 training dataframe with index Id\n         - 1460 target data series with index Id and name SalePrice\n    \"\"\"\n    input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir\n\n    raw_train_df = pd.read_csv(input_dir / \"train.csv\", index_col=\"Id\")\n    train_df = raw_train_df.drop(columns=\"SalePrice\")\n    target_ds = raw_train_df[\"SalePrice\"]\n\n    return train_df, target_ds\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.raw_test","title":"raw_test","text":"<pre><code>raw_test(input_dir=INPUT_DIR)\n</code></pre> <p>Load housing raw test data</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>             (<code>Path | str</code>, default:                 <code>INPUT_DIR</code> )         \u2013          <p>directory containing csv data, by default INPUT_DIR</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>1459 x 79 test dataframe with index Id</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def raw_test(input_dir: Path | str = INPUT_DIR):\n    \"\"\"Load housing raw test data\n\n    Parameters\n    ----------\n    input_dir : Path | str, optional\n        directory containing csv data, by default INPUT_DIR\n\n    Returns\n    -------\n    pd.DataFrame\n        1459 x 79 test dataframe with index Id\n    \"\"\"\n    input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir\n\n    test_df = pd.read_csv(input_dir / \"test.csv\", index_col=\"Id\")\n    return test_df\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.example_submission","title":"example_submission","text":"<pre><code>example_submission(input_dir=INPUT_DIR)\n</code></pre> <p>Load housing example submission</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>             (<code>Path | str</code>, default:                 <code>INPUT_DIR</code> )         \u2013          <p>directory containing csv data, by default INPUT_DIR</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Series</code>         \u2013          <p>1459 series containing example predictions on test data</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def example_submission(input_dir: Path | str = INPUT_DIR):\n    \"\"\"Load housing example submission\n\n    Parameters\n    ----------\n    input_dir : Path | str, optional\n        directory containing csv data, by default INPUT_DIR\n\n    Returns\n    -------\n    pd.Series\n        1459 series containing example predictions on test data\n    \"\"\"\n    input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir\n\n    submission_ds = pd.read_csv(\n        input_dir / \"sample_submission.csv\", index_col=\"Id\"\n    ).squeeze()\n    return submission_ds\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.transform_target","title":"transform_target","text":"<pre><code>transform_target(y)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def transform_target(y):\n    return np.log(y)\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.inv_transform_target","title":"inv_transform_target","text":"<pre><code>inv_transform_target(y)\n</code></pre> Source code in <code>housing/load_prep.py</code> <pre><code>def inv_transform_target(y):\n    return np.exp(y)\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.near_zero_var","title":"near_zero_var","text":"<pre><code>near_zero_var(X, *, freq_ratio=20, unique_pct=0.1, plot=False, return_stats=False)\n</code></pre> <p>Evaluates each feature for near zero variance.</p> <p>Feature is near zero variance if ratio of largest freq value to 2nd largest freq value is high and percentage of unique values is low</p> <p>Parameters:</p> <ul> <li> <code>X</code>             (<code>DataFrame</code>)         \u2013          </li> <li> <code>freq_ratio</code>             (<code>int</code>, default:                 <code>20</code> )         \u2013          <p>threshold for ratio of largest freq value to 2nd largest freq value, by default 20</p> </li> <li> <code>unique_pct</code>             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>threshold for percentage of unique values to total samples, by default 0.1</p> </li> <li> <code>plot</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>plot feature stats, by default False</p> </li> <li> <code>return_stats</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>return dataframe with stats if True, otherwise just return boolean series, by default False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Series | DataFrame</code>         \u2013          <p>boolean series indicating near zero var, otherwise dataframe with stats</p> </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def near_zero_var(\n    X: pd.DataFrame, *, freq_ratio=20, unique_pct=0.1, plot=False, return_stats=False\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"Evaluates each feature for near zero variance.\n\n    Feature is near zero variance if ratio of largest freq value\n    to 2nd largest freq value is high and percentage of unique values is low\n\n    Parameters\n    ----------\n    X : pd.DataFrame\n    freq_ratio : int, optional\n        threshold for ratio of largest freq value to 2nd largest freq value, by default 20\n    unique_pct : float, optional\n        threshold for percentage of unique values to total samples, by default 0.1\n    plot : bool, optional\n        plot feature stats, by default False\n    return_stats : bool, optional\n        return dataframe with stats if True, otherwise just return boolean series, by default False\n\n    Returns\n    -------\n    pd.Series | pd.DataFrame\n        boolean series indicating near zero var, otherwise dataframe with stats\n    \"\"\"\n\n    def _freq_ratio(col):\n        col_counts = col.value_counts(dropna=False)\n        return (\n            col_counts.iloc[0] / col_counts.iloc[1] if len(col_counts) &gt; 1 else math.inf\n        )\n\n    def _unique_pct(col):\n        return col.nunique() / len(col)\n\n    stat_agg = X.agg([_freq_ratio, _unique_pct], axis=0).T\n    nzvar = (stat_agg._freq_ratio &gt; freq_ratio) &amp; (stat_agg._unique_pct &lt; unique_pct)\n    stat_agg[\"near_zero_var\"] = nzvar\n    logger.info(\"{total} columns with near zero var\", total=sum(nzvar))\n    if return_stats:\n        nzvar = stat_agg\n\n    if plot:\n        sns.scatterplot(stat_agg, x=\"_unique_pct\", y=\"_freq_ratio\", hue=\"near_zero_var\")\n        plt.xscale(\"log\")\n        plt.yscale(\"log\")\n        plt.show()\n\n    return nzvar\n</code></pre>"},{"location":"reference/housing/load_prep/#housing.load_prep.preprocess_pipe","title":"preprocess_pipe","text":"<pre><code>preprocess_pipe()\n</code></pre> <p>Pipeline that filters near-zero variance columns and scales numeric data</p> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          </li> </ul> Source code in <code>housing/load_prep.py</code> <pre><code>def preprocess_pipe():\n    \"\"\"Pipeline that filters near-zero variance columns and scales numeric data\n\n    Returns\n    -------\n    Pipeline\n    \"\"\"\n    cat_scale = make_column_transformer(\n        (\n            # convert any object columns to categoricals\n            FunctionTransformer(\n                lambda X: X.astype(\"category\"), feature_names_out=\"one-to-one\"\n            ),\n            make_column_selector(dtype_include=\"object\"),\n        ),\n        # power transform all numeric columns\n        remainder=make_pipeline(StandardScaler(), PowerTransformer()).set_output(\n            transform=\"pandas\"\n        ),\n        verbose_feature_names_out=False\n    ).set_output(transform='pandas')\n\n    return make_pipeline(NearZeroVarSelector(), cat_scale)\n</code></pre>"},{"location":"reference/housing/model/","title":"model","text":"<p>Routines for housing model generation and evaluation.</p>"},{"location":"reference/housing/model/#housing.model.LGBMProxy","title":"LGBMProxy","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>LightGBM wrapper that conforms to sklearn interface</p> <p>specifically move callbacks and validation data to initialization rather than needing to be passed during the call to fit itself. Note catboost tries to JSON serialize all parameters (fails for dataframe)</p> <p>Parameters:</p> <ul> <li> <code>callbacks</code>             (<code>list</code>, default:                 <code>None</code> )         \u2013          <p>lightgbm fit callback functions</p> </li> <li> <code>validation</code>             (<code>tuple(X, y)</code>, default:                 <code>None</code> )         \u2013          <p>validation data, required to use early stopping</p> </li> <li> <code>**params</code>             (<code>optional</code>, default:                 <code>{}</code> )         \u2013          <p>parameters to be passed to LGBMRegressor initialization</p> </li> </ul> Source code in <code>housing/model.py</code> <pre><code>class LGBMProxy(BaseEstimator, RegressorMixin):\n    \"\"\"LightGBM wrapper that conforms to sklearn interface\n\n    specifically move callbacks and validation data to initialization\n    rather than needing to be passed during the call to fit itself.\n    Note catboost tries to JSON serialize all parameters (fails for dataframe)\n\n    Parameters\n    ----------\n    callbacks : list, optional\n        lightgbm fit callback functions\n    validation : tuple (X, y), optional\n        validation data, required to use early stopping\n    **params : optional\n        parameters to be passed to LGBMRegressor initialization\n    \"\"\"\n\n    def __init__(self, callbacks=None, validation=None, **params):\n        self.callbacks = callbacks\n        self.validation = validation\n        # lightgbm classifier\n        self.estimator_ = lgbm.LGBMRegressor(**params)\n\n    def get_params(self, deep=True):\n        return self.estimator_.get_params(deep) | {\n            \"callbacks\": self.callbacks,\n            \"validation\": self.validation,\n        }\n\n    def set_params(self, **params):\n        if \"callbacks\" in params:\n            self.callbacks = params.pop(\"callbacks\")\n        if \"validation\" in params:\n            self.validation = params.pop(\"validation\")\n        self.estimator_.set_params(**params)\n\n    def fit(self, X, y):\n        if self.validation is not None:\n            eval_set = [self.validation, (X, y)]\n            eval_names = [\"validation\", \"training\"]\n        else:\n            eval_set = None\n            eval_names = None\n        self.estimator_.fit(\n            X,\n            y,\n            eval_set=eval_set,\n            eval_names=eval_names,\n            eval_metric=None,  # defaulting to training objective\n            callbacks=self.callbacks,\n        )\n        return self\n\n    def __getattr__(self, name):\n        \"\"\"dispatch other methods to estimator\"\"\"\n        return getattr(self.estimator_, name)\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.callbacks","title":"callbacks  <code>instance-attribute</code>","text":"<pre><code>callbacks = callbacks\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.validation","title":"validation  <code>instance-attribute</code>","text":"<pre><code>validation = validation\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.estimator_","title":"estimator_  <code>instance-attribute</code>","text":"<pre><code>estimator_ = lgbm.LGBMRegressor(**params)\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.__init__","title":"__init__","text":"<pre><code>__init__(callbacks=None, validation=None, **params)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def __init__(self, callbacks=None, validation=None, **params):\n    self.callbacks = callbacks\n    self.validation = validation\n    # lightgbm classifier\n    self.estimator_ = lgbm.LGBMRegressor(**params)\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.get_params","title":"get_params","text":"<pre><code>get_params(deep=True)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def get_params(self, deep=True):\n    return self.estimator_.get_params(deep) | {\n        \"callbacks\": self.callbacks,\n        \"validation\": self.validation,\n    }\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.set_params","title":"set_params","text":"<pre><code>set_params(**params)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def set_params(self, **params):\n    if \"callbacks\" in params:\n        self.callbacks = params.pop(\"callbacks\")\n    if \"validation\" in params:\n        self.validation = params.pop(\"validation\")\n    self.estimator_.set_params(**params)\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def fit(self, X, y):\n    if self.validation is not None:\n        eval_set = [self.validation, (X, y)]\n        eval_names = [\"validation\", \"training\"]\n    else:\n        eval_set = None\n        eval_names = None\n    self.estimator_.fit(\n        X,\n        y,\n        eval_set=eval_set,\n        eval_names=eval_names,\n        eval_metric=None,  # defaulting to training objective\n        callbacks=self.callbacks,\n    )\n    return self\n</code></pre>"},{"location":"reference/housing/model/#housing.model.LGBMProxy.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>dispatch other methods to estimator</p> Source code in <code>housing/model.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"dispatch other methods to estimator\"\"\"\n    return getattr(self.estimator_, name)\n</code></pre>"},{"location":"reference/housing/model/#housing.model.get_imputer","title":"get_imputer","text":"<pre><code>get_imputer(impute_categoricals=True)\n</code></pre> <p>Pipeline step to impute scaled and filtered data.</p> <p>Data is ordinal encoded with missing indicators and scaled numeric after thsi step.</p> <p>Parameters:</p> <ul> <li> <code>impute_categoricals</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to impute categorical features or just encode missing, by default True</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          </li> </ul> Source code in <code>housing/model.py</code> <pre><code>def get_imputer(impute_categoricals=True):\n    \"\"\"Pipeline step to impute scaled and filtered data.\n\n    Data is ordinal encoded with missing indicators and scaled numeric after thsi step.\n\n    Parameters\n    ----------\n    impute_categoricals : bool, optional\n        whether to impute categorical features or just encode missing, by default True\n\n    Returns\n    -------\n    Pipeline\n    \"\"\"\n    if impute_categoricals:\n        # ordinal encoding followed by categorical naive bayes impute\n        # unknown categories are marked missing and imputed (catNB can't handle -1)\n        cat_impute = make_pipeline(\n            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan),\n            IterativeImputer(\n                CategoricalNB(min_categories=25),\n                initial_strategy=\"most_frequent\",\n                add_indicator=True,\n                skip_complete=True,\n            ),\n        ).set_output(transform=\"pandas\")\n    else:\n        # ordinal encoding with unknown/missing encoded as -1\n        cat_impute = OrdinalEncoder(\n            handle_unknown=\"use_encoded_value\",\n            unknown_value=-1,\n            encoded_missing_value=-1,\n        )\n\n    # cat_step contains names of original categorical and numeric column names\n    cat_step = make_column_transformer(\n        (cat_impute, make_column_selector(dtype_include=\"category\")),\n        (\"passthrough\", make_column_selector(dtype_exclude=\"category\")),\n        verbose_feature_names_out=False,\n    )\n\n    num_step = make_union(\n        # pass ordinal encoded categoricals\n        make_column_transformer(\n            (\"passthrough\", lambda X: cat_step.transformers_[0][2]),\n            remainder=\"drop\",\n            verbose_feature_names_out=False,\n        ),\n        # impute numeric and pass missing indicators\n        make_pipeline(\n            make_column_transformer(\n                (\n                    OneHotEncoder(\n                        handle_unknown=\"infrequent_if_exist\",\n                        min_frequency=5,\n                        sparse_output=False,\n                    ),\n                    lambda X: cat_step.transformers_[0][2],\n                ),\n                remainder=\"passthrough\",\n                verbose_feature_names_out=False,\n            ),\n            IterativeImputer(BayesianRidge(), skip_complete=True, add_indicator=True),\n            make_column_transformer(\n                (\"passthrough\", lambda X: cat_step.transformers_[1][2]),\n                (\"passthrough\", make_column_selector(pattern=\"missingindicator*\")),\n                remainder=\"drop\",\n                verbose_feature_names_out=False,\n            ),\n        ),\n    )\n    # removes prefix append by feature union\n    fix_col_names = FunctionTransformer(\n        lambda X: X.set_axis(\n            list(pd.Series(X.columns).str.split(\"__\", expand=True)[1]), axis=1\n        )\n    )\n\n    return make_pipeline(cat_step, num_step, fix_col_names).set_output(\n        transform=\"pandas\"\n    )\n</code></pre>"},{"location":"reference/housing/model/#housing.model.get_pca_step","title":"get_pca_step","text":"<pre><code>get_pca_step(ordinal_selector, numeric_selector, passthrough=True, cat_n_components=5, num_n_components=5)\n</code></pre> <p>Apply PCA transform to one-hot categoricals and numeric features.</p> <p>Input is ordinal imputed data. Relies on selector funcs to identify remaining ordinal columns and numeric columns. PCA applied separately to categoricals and numeric and concatenated</p> <p>Parameters:</p> <ul> <li> <code>ordinal_selector</code>             (<code>Callable[[X], columns]</code>)         \u2013          </li> <li> <code>numeric_selector</code>             (<code>Callable[[X], columns]</code>)         \u2013          </li> <li> <code>passthrough</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to concat input data to PCA output, by default True</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          </li> </ul> Source code in <code>housing/model.py</code> <pre><code>def get_pca_step(\n    ordinal_selector,\n    numeric_selector,\n    passthrough=True,\n    cat_n_components=5,\n    num_n_components=5,\n):\n    \"\"\"Apply PCA transform to one-hot categoricals and numeric features.\n\n    Input is ordinal imputed data. Relies on selector funcs to identify\n    remaining ordinal columns and numeric columns. PCA applied separately\n    to categoricals and numeric and concatenated\n\n    Parameters\n    ----------\n    ordinal_selector : Callable[[X], columns]\n    numeric_selector : Callable[[X], columns]\n    passthrough : bool, optional\n        whether to concat input data to PCA output, by default True\n\n    Returns\n    -------\n    Pipeline\n    \"\"\"\n    cat_pca = make_pipeline(\n        make_column_transformer(\n            (OneHotEncoder(sparse_output=False), ordinal_selector),\n            (\"drop\", numeric_selector),\n            remainder=\"passthrough\",\n            verbose_feature_names_out=False,\n        ),\n        PCA(n_components=cat_n_components),\n    ).set_output(transform=\"pandas\")\n\n    num_pca = make_pipeline(\n        make_column_transformer(\n            (\"passthrough\", numeric_selector),\n            remainder=\"drop\",\n            verbose_feature_names_out=False,\n        ),\n        PCA(n_components=num_n_components),\n    ).set_output(transform=\"pandas\")\n\n    # removes prefix append by feature union\n    # slight trick to only remove prefix from passthrough data\n    fix_col_names = FunctionTransformer(\n        lambda X: X.set_axis(\n            list(pd.Series(X.columns).str.removeprefix(\"orig__\")), axis=1\n        )\n    )\n\n    union_steps = [(\"orig\", \"passthrough\")] if passthrough else []\n    union_steps += [(\"cat\", cat_pca), (\"num\", num_pca)]\n    return make_pipeline(FeatureUnion(union_steps), fix_col_names).set_output(\n        transform=\"pandas\"\n    )\n</code></pre>"},{"location":"reference/housing/model/#housing.model.get_pca_pipeline","title":"get_pca_pipeline","text":"<pre><code>get_pca_pipeline(cat_n_components=5, num_n_components=5)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def get_pca_pipeline(cat_n_components=5, num_n_components=5):\n    # raw to PCs\n    # near zero var, scale, impute, PCA\n    impute_step = get_imputer()\n\n    pca_pipe = make_pipeline(\n        load_prep.preprocess_pipe(),\n        impute_step,\n        get_pca_step(\n            _get_ordinal_selector(impute_step),\n            _get_numeric_selector(impute_step),\n            passthrough=False,\n            cat_n_components=cat_n_components,\n            num_n_components=num_n_components,\n        ),\n    )\n    return pca_pipe\n</code></pre>"},{"location":"reference/housing/model/#housing.model.get_gower_dist","title":"get_gower_dist","text":"<pre><code>get_gower_dist(X)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def get_gower_dist(X):\n    # near zero var, scale, impute\n    # impute to one hot cats to dice\n    # impute to numeric to range normalized manhattan\n    impute_step = get_imputer()\n\n    impute_pipe = make_pipeline(load_prep.preprocess_pipe(), impute_step)\n\n    # one hot encode remaining ordinal features\n    impute_to_onehot = make_column_transformer(\n        (OneHotEncoder(sparse_output=False), _get_ordinal_selector(impute_step)),\n        (\"drop\", _get_numeric_selector(impute_step)),\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n    # scale numeric to [0, 1]\n    impute_to_num = make_column_transformer(\n        (MinMaxScaler(), _get_numeric_selector(impute_step)),\n        remainder=\"drop\",\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n\n    X_imputed = impute_pipe.fit_transform(X)\n    X_onehot = impute_to_onehot.fit_transform(X_imputed)\n    X_num = impute_to_num.fit_transform(X_imputed)\n\n    dice_dist = pairwise_distances(X_onehot.to_numpy(), metric=\"dice\")\n    man_dist = pairwise_distances(X_num, metric=\"cityblock\") / X_num.shape[1]\n\n    return (dice_dist + man_dist) / 2.0\n</code></pre>"},{"location":"reference/housing/model/#housing.model.get_regressor","title":"get_regressor","text":"<pre><code>get_regressor(strategy='passthrough', params=None)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def get_regressor(strategy=\"passthrough\", params=None):\n    params = params if params is not None else {}\n\n    if strategy == \"passthrough\":\n        return \"passthrough\"\n    elif strategy == \"linear\":\n        enet_params = {\"l1_ratio\": 0.9}\n        params = enet_params | params\n        return ElasticNetCV(**params)\n    elif strategy == \"lightgbm\":\n        return LGBMProxy(**params)\n    elif strategy == \"flaml\":\n        return flaml.AutoML(**params)\n    elif strategy == 'autogluon':\n        return AGProxy(**params)\n    elif strategy == \"custom\":\n        return params['estimator']\n    else:\n        raise ValueError(f\"unimplemented strategy {strategy}\")\n</code></pre>"},{"location":"reference/housing/model/#housing.model.get_reg_pipeline","title":"get_reg_pipeline","text":"<pre><code>get_reg_pipeline(reg_strategy='passthrough', reg_params=None, onehot_encode=False, include_pca=True, as_category=False)\n</code></pre> Source code in <code>housing/model.py</code> <pre><code>def get_reg_pipeline(\n    reg_strategy=\"passthrough\",\n    reg_params=None,\n    onehot_encode=False,\n    include_pca=True,\n    as_category=False,\n):\n    reg_params = reg_params if reg_params is not None else {}\n\n    # preprocess, impute, (optional) onehot, regressor\n    impute_step = get_imputer()\n\n    # PCA step\n    pca_step = get_pca_step(\n        _get_ordinal_selector(impute_step),\n        _get_numeric_selector(impute_step),\n        passthrough=True,\n    )\n\n    # one hot encode remaining ordinal features\n    impute_to_onehot = make_column_transformer(\n        (OneHotEncoder(sparse_output=False), _get_ordinal_selector(impute_step)),\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n\n    # cast features to categorical before input to regressor\n    cast_categoricals = make_column_transformer(\n        (\"passthrough\", _get_numeric_selector(impute_step)),\n        (\"passthrough\", make_column_selector(\".*pca.*\")),\n        remainder=FunctionTransformer(lambda X: X.astype(\"category\")),\n        verbose_feature_names_out=False\n    ).set_output(transform=\"pandas\")\n\n    reg_pipe = Pipeline(\n        [\n            (\"preprocess\", load_prep.preprocess_pipe()),\n            (\"impute\", impute_step),\n            (\"PCA\", pca_step if include_pca else \"passthrough\"),\n            (\"onehot\", impute_to_onehot if onehot_encode else \"passthrough\"),\n            (\"categoricals\", cast_categoricals if as_category else \"passthrough\"),\n            (\"regressor\", get_regressor(strategy=reg_strategy, params=reg_params)),\n        ]\n    )\n    # since we are passing info between steps, we override clone\n    reg_pipe.__sklearn_clone__ = lambda: get_reg_pipeline(\n        reg_strategy, reg_params, onehot_encode, include_pca, as_category\n    )\n    return reg_pipe\n</code></pre>"},{"location":"reference/housing/model/#housing.model.fit_with_validation","title":"fit_with_validation","text":"<pre><code>fit_with_validation(reg_pipe, X_train, y_train, X_valid=None, y_valid=None)\n</code></pre> <p>fit routine for regression pipeline.</p> <p>fits first stages first in order to pass transformed validation data to final classifier if necessary</p> <p>Parameters:</p> <ul> <li> <code>reg_pipe</code>             (<code>Pipeline</code>)         \u2013          <p>multi-step pipeline with final step regressor</p> </li> <li> <code>X_train</code>             (<code>2D array_like</code>)         \u2013          </li> <li> <code>y_train</code>             (<code>1D array_like</code>)         \u2013          </li> <li> <code>X_valid</code>             (<code>2D array_like</code>, default:                 <code>None</code> )         \u2013          <p>validation data to be transformed and passed to regressor, by default None</p> </li> <li> <code>y_valid</code>             (<code>1D array_like</code>, default:                 <code>None</code> )         \u2013          <p>by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>reg_pipe</code> (            <code>(Pipeline, fitted)</code> )        \u2013          </li> </ul> Source code in <code>housing/model.py</code> <pre><code>def fit_with_validation(\n    reg_pipe: Pipeline,\n    X_train,\n    y_train,\n    X_valid=None,\n    y_valid=None,\n):\n    \"\"\"fit routine for regression pipeline.\n\n    fits first stages first in order to pass transformed\n    validation data to final classifier if necessary\n\n    Parameters\n    ----------\n    reg_pipe : Pipeline\n        multi-step pipeline with final step regressor\n    X_train : 2D array_like\n    y_train : 1D array_like\n    X_valid : 2D array_like, optional\n        validation data to be transformed and passed to regressor, by default None\n    y_valid : 1D array_like, optional\n        by default None\n\n    Returns\n    -------\n    reg_pipe : Pipeline, fitted\n    \"\"\"\n    if X_valid is None:\n        # simple call to fit\n        return reg_pipe.fit(X_train, y_train)\n\n    X_train_pre = reg_pipe[:-1].fit_transform(X_train, y_train)\n    X_valid_pre = reg_pipe[:-1].transform(X_valid)\n\n    # last step has to accept validation as a param\n    reg_pipe[-1].set_params(validation=(X_valid_pre, y_valid))\n    reg_pipe[-1].fit(X_train_pre, y_train)\n\n    return reg_pipe\n</code></pre>"},{"location":"reference/housing/model/#housing.model.cv_with_validation","title":"cv_with_validation","text":"<pre><code>cv_with_validation(estimator, X, y, cv, callbacks=None)\n</code></pre> <p>Perform cross-validation while passing validation data to the estimator in each fold.</p> <p>estimator is fit using <code>fit_with_validation</code>, and is assumed to be a pipeline that can accept a validation data parameter</p> <p>return results dictionary with one key per callback</p> Source code in <code>housing/model.py</code> <pre><code>def cv_with_validation(estimator, X, y, cv, callbacks=None):\n    \"\"\"Perform cross-validation while passing validation data to the estimator\n    in each fold.\n\n    estimator is fit using `fit_with_validation`, and is assumed to be a pipeline that\n    can accept a validation data parameter\n\n    return results dictionary with one key per callback\n    \"\"\"\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n    callbacks = callbacks if callbacks is not None else {}\n\n    result = {k: {} for k in callbacks.keys()}\n    result[\"train_time\"] = {}\n\n    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        fold_estimator = clone(estimator)\n        # form a dummy pipeline to make this compatible with single estimators\n        if not isinstance(estimator, Pipeline):\n            fold_estimator = make_pipeline(\"passthrough\", fold_estimator)\n\n        X_train, y_train = _safe_split(fold_estimator, X, y, train_idx)\n        X_valid, y_valid = _safe_split(\n            fold_estimator, X, y, test_idx, train_indices=train_idx\n        )\n\n        start_time = time()\n\n        if \"validation\" in fold_estimator[-1].get_params():\n            # pass validation data if validation param exists in final pipeline step\n            fit_with_validation(fold_estimator, X_train, y_train, X_valid, y_valid)\n        else:\n            fit_with_validation(fold_estimator, X_train, y_train)\n\n        fit_time = time() - start_time\n        result[\"train_time\"][fold] = fit_time\n\n        if not isinstance(estimator, Pipeline):\n            # revert back to single estimator for evaluation\n            fold_estimator = fold_estimator[-1]\n\n        for k, func in callbacks.items():\n            result[k][fold] = func(\n                fold=fold,\n                estimator=fold_estimator,\n                indices=(train_idx, test_idx),\n                train_data=(X_train, y_train),\n                test_data=(X_valid, y_valid),\n            )\n    return result\n</code></pre>"},{"location":"reference/housing/model/#housing.model.lgbm_fit_metrics","title":"lgbm_fit_metrics","text":"<pre><code>lgbm_fit_metrics(*, estimator, **kwargs)\n</code></pre> <p>Return fit metrics for fitted LightGBM model</p> Source code in <code>housing/model.py</code> <pre><code>def lgbm_fit_metrics(*, estimator, **kwargs):\n    \"\"\"Return fit metrics for fitted LightGBM model\"\"\"\n    reg = estimator[\"regressor\"]\n    best_ntree = reg.best_iteration_ if reg.best_iteration_ else reg.n_estimators\n    best_idx = best_ntree - 1\n\n    lgbm_evals = {\n        **{\"train_\" + k: v[best_idx] for k, v in reg.evals_result_[\"training\"].items()},\n        **{\n            \"test_\" + k: v[best_idx] for k, v in reg.evals_result_[\"validation\"].items()\n        },\n        **{\"best_ntree\": best_ntree},\n    }\n    return pd.DataFrame(pd.Series(lgbm_evals)).T\n</code></pre>"},{"location":"reference/housing/model/#housing.model.common_cv_callbacks","title":"common_cv_callbacks","text":"<pre><code>common_cv_callbacks()\n</code></pre> <p>Generates dictionary of common CV callback functions for cv_with_validation.</p> <p>includes:     - fitted estimator     - fold indices     - evaluation metrics on training data     - evaluation metrics on test data     - test predictions     - test target data</p> Source code in <code>housing/model.py</code> <pre><code>def common_cv_callbacks():\n    \"\"\"Generates dictionary of common CV callback functions for cv_with_validation.\n\n    includes:\n        - fitted estimator\n        - fold indices\n        - evaluation metrics on training data\n        - evaluation metrics on test data\n        - test predictions\n        - test target data\n    \"\"\"\n    callbacks = {\n        \"estimator\": lambda *, estimator, **kw: estimator,\n        \"indices\": lambda *, indices, **kw: indices,\n        \"eval_train\": _score_train,\n        \"eval_test\": _score_test,\n        \"predict_test\": _predict_test,\n        \"y_test\": lambda *, test_data, **kw: test_data[1],\n    }\n    return callbacks\n</code></pre>"},{"location":"reference/housing/utils/","title":"utils","text":"<p>General package utilities</p>"},{"location":"reference/housing/utils/#housing.utils.ON_KAGGLE","title":"ON_KAGGLE  <code>module-attribute</code>","text":"<pre><code>ON_KAGGLE = os.environ['PWD'] == '/kaggle/working'\n</code></pre>"},{"location":"reference/housing/utils/#housing.utils.INPUT_DIR","title":"INPUT_DIR  <code>module-attribute</code>","text":"<pre><code>INPUT_DIR = Path('/kaggle/input/house-prices-advanced-regression-techniques') if ON_KAGGLE else Path('/Users/herman/Dropbox/ml_practice/classic_ml/housing/data/raw')\n</code></pre>"},{"location":"reference/housing/utils/#housing.utils.WORKING_DIR","title":"WORKING_DIR  <code>module-attribute</code>","text":"<pre><code>WORKING_DIR = Path('/kaggle/working') if ON_KAGGLE else Path('/Users/herman/Dropbox/ml_practice/classic_ml/housing')\n</code></pre>"}]}